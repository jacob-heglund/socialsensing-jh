{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Notebook for Analyzing Sandy Twitter Data (from mdredze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/httran/Documents/projects/twitterinfrastructure\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from twitterinfrastructure.tools import dump, output\n",
    "\n",
    "import twitterinfrastructure.twitter_sandy as ts\n",
    "import importlib\n",
    "importlib.reload(ts)\n",
    "\n",
    "#os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pre-processing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'extended_entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n\n"
     ]
    }
   ],
   "source": [
    "# test tweet field names\n",
    "collection = 'tweets_analysis'\n",
    "tweet_collection = 'tweets'\n",
    "zones_collection = 'taxi_zones'\n",
    "fields = ['_id', 'coordinates', 'created_at', 'entities', 'full_text', \n",
    "          'id_str', 'place']\n",
    "db_name = 'sandy'\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "\n",
    "zones_iter = db[zones_collection].find()\n",
    "for zone in zones_iter[0:1]:\n",
    "    # query tweets within current taxi zone\n",
    "    query_dict = {\n",
    "        \"coordinates\": {\n",
    "            \"$geoWithin\": {\n",
    "                \"$geometry\": zone['geometry']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    full_tweets = db[tweet_collection].find(query_dict)\n",
    "    \n",
    "print(full_tweets.count())\n",
    "full_tweets = list(full_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n[{'indices': [93, 109], 'text': 'reyesenterprise'}, {'indices': [110, 127], 'text': 'workhardplayhard'}]\nWith my dad Going to delaware with 10 of my dads trucks and 1 crane for emergency generators #reyesenterprise #workhardplayhard\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'extended_entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'possibly_sensitive', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n[]\nJust landed in Newark, next stop Berlin then on to Romania. Getting excited but could do without the 10.5 hour flight http://t.co/NMwHniu4\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n[{'indices': [3, 22], 'text': 'atlanticexpressbus'}]\nIn #atlanticexpressbus 315 inbound to NYC via Lincoln Tunnel. The driver is really aggressive in driving today!\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n[]\nI took today off, I won't be cutting!\n\ndict_keys(['_id', 'contributors', 'coordinates', 'created_at', 'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place', 'retweet_count', 'retweeted', 'source', 'text', 'truncated', 'user'])\n[{'indices': [61, 76], 'text': 'hungryandtired'}]\nLanded at newark! Time to breath in that jersey air....mmmmm #hungryandtired\n\n"
     ]
    }
   ],
   "source": [
    "for full_tweet in full_tweets[0:5]:\n",
    "    print(full_tweet.keys())\n",
    "    print(full_tweet['entities']['hashtags'])\n",
    "    print(full_tweet['text'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5b10b7000156950952b399d9'), 'contributors': None, 'coordinates': {'coordinates': [-73.95361427, 40.68187311], 'type': 'Point'}, 'created_at': 'Thu Nov 01 04:57:07 +0000 2012', 'entities': {'hashtags': [], 'symbols': [], 'urls': [], 'user_mentions': [{'id': 383956775, 'id_str': '383956775', 'indices': [0, 11], 'name': 'kash', 'screen_name': 'iamkashema'}]}, 'favorite_count': 0, 'favorited': False, 'geo': {'coordinates': [40.68187311, -73.95361427], 'type': 'Point'}, 'id': 263867240783425540, 'id_str': '263867240783425536', 'in_reply_to_screen_name': 'iamkashema', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': 383956775, 'in_reply_to_user_id_str': '383956775', 'is_quote_status': False, 'lang': 'en', 'place': {'attributes': {}, 'bounding_box': {'coordinates': [[[-74.255641, 40.495865], [-73.699793, 40.495865], [-73.699793, 40.91533], [-74.255641, 40.91533]]], 'type': 'Polygon'}, 'contained_within': [], 'country': 'United States', 'country_code': 'US', 'full_name': 'New York, NY', 'id': '27485069891a7938', 'name': 'New York', 'place_type': 'admin', 'url': 'https://api.twitter.com/1.1/geo/id/27485069891a7938.json'}, 'retweet_count': 0, 'retweeted': False, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'text': \"@iamkashema I'm mad u didn't ring down my phone to find out. People, including my drivers was callin me all day for that. I hope u got thru\", 'truncated': False, 'user': {'contributors_enabled': False, 'created_at': 'Mon Mar 07 18:50:21 +0000 2011', 'default_profile': False, 'default_profile_image': False, 'description': 'IG: instadread  SC:instadread', 'entities': {'description': {'urls': []}, 'url': {'urls': [{'display_url': 'facebook.com/ClydeAutoDealer', 'expanded_url': 'http://www.facebook.com/ClydeAutoDealer', 'indices': [0, 23], 'url': 'https://t.co/q9VdOBRRDh'}]}}, 'favourites_count': 219, 'follow_request_sent': False, 'followers_count': 290, 'following': False, 'friends_count': 588, 'geo_enabled': True, 'has_extended_profile': True, 'id': 262276763, 'id_str': '262276763', 'is_translation_enabled': False, 'is_translator': False, 'lang': 'en', 'listed_count': 6, 'location': 'East Coast', 'name': 'L Express', 'notifications': False, 'profile_background_color': '1A1B1F', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme9/bg.gif', 'profile_background_tile': False, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/262276763/1442383011', 'profile_image_url': 'http://pbs.twimg.com/profile_images/824637001131556864/3pNbHCUv_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/824637001131556864/3pNbHCUv_normal.jpg', 'profile_link_color': '2FC2EF', 'profile_sidebar_border_color': '181A1E', 'profile_sidebar_fill_color': '252429', 'profile_text_color': '666666', 'profile_use_background_image': True, 'protected': False, 'screen_name': 'RideMyTaxi', 'statuses_count': 8243, 'time_zone': 'Quito', 'translator_type': 'none', 'url': 'https://t.co/q9VdOBRRDh', 'utc_offset': -18000, 'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "# test duplicate key error\n",
    "# due to tweets found in multiple zones?\n",
    "collection = 'tweets_analysis'\n",
    "tweet_collection = 'tweets'\n",
    "zones_collection = 'taxi_zones'\n",
    "fields = ['_id', 'coordinates', 'created_at', 'entities', 'full_text', \n",
    "          'id_str', 'place']\n",
    "db_name = 'sandy'\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "\n",
    "tweet = db[tweet_collection].find_one({'id_str': '263867240783425536'})\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\nClinton Hill\nBedford\n"
     ]
    }
   ],
   "source": [
    "# test duplicate key error\n",
    "# query zones intersecting with current tweet (matches 2)\n",
    "query_dict = {\n",
    "    \"geometry\": {\n",
    "        \"$geoIntersects\": {\n",
    "            \"$geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": tweet['coordinates']['coordinates'] \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "zones = db[zones_collection].find(query_dict)\n",
    "print(zones.count())\n",
    "for zone in zones:\n",
    "    print(zone['properties']['zone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet[created_at]: Sun Oct 28 22:42:35 +0000 2012\nconverted to utc_time:\n2012-10-28 22:42:35+00:00\ndocument to be inserted:\n{'datetimeUTC': datetime.datetime(2012, 10, 28, 22, 42, 35, tzinfo=datetime.timezone.utc), 'timestampUNIX': 1351464155000}\n\naggregation retrieved from collection:\n[{'_id': {'month': 10, 'day': 28, 'year': 2012, 'hour': 22}, 'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "# test mongodb and pymongo datetime conversions\n",
    "# mongodb stores as UTC, but will display in MongoDB Compass based on current\n",
    "# timezone\n",
    "analysis_collection = 'tweets_sandy'\n",
    "db_name = 'sandy'\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "\n",
    "tweet = db[analysis_collection].find_one()\n",
    "utc_time = dt.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S %z %Y')\n",
    "print('tweet[created_at]: ' + tweet['created_at'])\n",
    "print('converted to utc_time:')\n",
    "print(utc_time)\n",
    "\n",
    "doc = {}\n",
    "doc['datetimeUTC'] = utc_time\n",
    "doc['timestampUNIX'] = int(\n",
    "    utc_time.replace(tzinfo=dt.timezone.utc).timestamp() * 1000)\n",
    "#doc['date'] = utc_time.date()\n",
    "#doc['hour'] = utc_time.hour\n",
    "print('document to be inserted:')\n",
    "print(doc)\n",
    "print('')\n",
    "\n",
    "collection = 'datetime_test'\n",
    "db.drop_collection(collection)\n",
    "db[collection].insert_one(doc)\n",
    "result1 = db[collection].aggregate([\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": {\n",
    "                \"month\": {\"$month\": \"$datetimeUTC\"},\n",
    "                \"day\": {\"$dayOfMonth\": \"$datetimeUTC\"},\n",
    "                \"year\": {\"$year\": \"$datetimeUTC\"},\n",
    "                \"hour\": {\"$hour\": \"$datetimeUTC\"}\n",
    "            },\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }\n",
    "    }\n",
    "])\n",
    "print('aggregation retrieved from collection:')\n",
    "print(list(result1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test create_analysis\n",
    "import twitterinfrastructure.twitter_sandy as ts\n",
    "import importlib\n",
    "importlib.reload(ts)\n",
    "\n",
    "collection = 'tweets_analysis'\n",
    "tweet_collection = 'tweets'\n",
    "nyisozones_collection = 'nyiso_zones'\n",
    "taxizones_collection = 'taxi_zones'\n",
    "fields = ['_id', 'coordinates', 'created_at', 'entities', 'text', \n",
    "          'id_str', 'place']\n",
    "db_name = 'sandy'\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "progressbar = False\n",
    "overwrite = True\n",
    "verbose = 1\n",
    "\n",
    "# import_num, tokens, full_tweets = ts.create_analysis(collection=collection, \n",
    "#                                         tweet_collection=tweet_collection, \n",
    "#                                         nyisozones_collection=nyisozones_collection,\n",
    "#                                         taxizones_collection=taxizones_collection,\n",
    "#                                         fields=fields, db_name=db_name, \n",
    "#                                         db_instance=db_instance, \n",
    "#                                         progressbar=False, \n",
    "#                                         overwrite=overwrite, verbose=verbose)\n",
    "\n",
    "# connect to db (creates if not exists)\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "\n",
    "# ensure that nyisozones_collection and taxizones_collection exist\n",
    "collections = db.collection_names()\n",
    "if (nyisozones_collection not in collections) or \\\n",
    "        (taxizones_collection not in collections):\n",
    "    output('{nyiso} or {taxi} collection not in database. No action '\n",
    "           'taken.'.format(nyiso=nyisozones_collection,\n",
    "                           taxi=taxizones_collection))\n",
    "\n",
    "# process and insert tweets\n",
    "full_tweets = db[tweet_collection].find()\n",
    "if progressbar:\n",
    "    tweets_iter = tqdm(full_tweets, total=full_tweets.count(),\n",
    "                       desc='tweets', leave=False)\n",
    "else:\n",
    "    tweets_iter = full_tweets\n",
    "full_tweet = tweets_iter[0]\n",
    "# for full_tweet in tweets_iter:\n",
    "# remove extra fields\n",
    "if fields:\n",
    "    tweet = {field: full_tweet[field] for field in fields}\n",
    "else:\n",
    "    tweet = full_tweet\n",
    "\n",
    "# identify and add nyiso zone, taxi zone, and taxi borough\n",
    "if tweet['coordinates'] is not None:\n",
    "    query_dict = {\n",
    "        \"geometry\": {\n",
    "            \"$geoIntersects\": {\n",
    "                \"$geometry\": tweet['coordinates']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    nyiso_zone = db[nyisozones_collection].find_one(query_dict)\n",
    "    if nyiso_zone:\n",
    "        tweet['nyiso_zone'] = nyiso_zone['properties']['Zone']\n",
    "    else:\n",
    "        tweet['nyiso_zone'] = np.nan\n",
    "\n",
    "    query_dict = {\n",
    "        \"geometry\": {\n",
    "            \"$geoIntersects\": {\n",
    "                \"$geometry\": tweet['coordinates']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    taxi_zone = db[taxizones_collection].find_one(query_dict)\n",
    "    if taxi_zone:\n",
    "        tweet['location_id'] = taxi_zone['properties']['LocationID']\n",
    "        tweet['borough'] = taxi_zone['properties']['borough']\n",
    "    else:\n",
    "        tweet['location_id'] = np.nan\n",
    "        tweet['borough'] = np.nan\n",
    "else:\n",
    "    # fails.append(tweet['id_str'])\n",
    "    if verbose >= 2:\n",
    "        output('Tweet skipped due to missing coordinates',\n",
    "               'create_analysis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@NOT_savinHOES Not r yu upp\n",
      "['r', 'yu', 'upp']\n",
      "1350882000.0\n",
      "\n",
      "Who's up?\n",
      "[\"who'\"]\n",
      "1350882000.0\n",
      "\n",
      "@augustushazel idk I'm just ugly or annoying or something\n",
      "['idk', \"i'm\", 'ugli', 'annoy', 'someth']\n",
      "1350882000.0\n",
      "\n",
      "@InYurMomsCovers your cool thou. You seem like one of those girls that you can talk about anything to. Not too many are like that\n",
      "['cool', 'thou', 'seem', 'like', 'one', 'girl', 'talk', 'anyth', 'mani', 'like']\n",
      "1350882001.0\n",
      "\n",
      "\"I suppose she has an appropriate costume for every activity...\" #ilovemaggiesmith #downtonseasonthree\n",
      "['suppos', 'appropri', 'costum', 'everi', 'activ', '...', '#ilovemaggiesmith', '#downtonseasonthre']\n",
      "1350882000.0\n",
      "\n",
      "Hit and Run is so sad..\n",
      "['hit', 'run', 'sad', '..']\n",
      "1350882000.0\n",
      "\n",
      "10/22 @ 01:00 - Temperature 46.6F. Wind 1.5mph SW. Gust 3.8mph. Barometer 30.06in, Steady. Rain today 0.00in. Humidity 92%.\n",
      "['10/22', '01:00', 'temperatur', '46.6', 'F', 'wind', '1.5', 'mph', 'SW', 'gust', '3.8', 'mph', 'baromet', '30.06', 'steadi', 'rain', 'today', '0.00', 'humid', '92']\n",
      "1350882001.0\n",
      "\n",
      "A bitch nigga, that's that shit I don't like, nah\n",
      "Sneak dissers, that's that shit I don't like\n",
      "['bitch', 'nigga', \"that'\", 'shit', 'like', 'nah', 'sneak', 'disser', \"that'\", 'shit', 'like']\n",
      "1350882001.0\n",
      "\n",
      "{'steadi', '1.5', 'wind', 'one', 'thou', '..', 'SW', 'hit', '...', '01:00', 'temperatur', 'sad', '30.06', 'cool', 'nigga', 'upp', 'mani', 'girl', 'bitch', 'costum', '92', \"that'\", 'humid', \"i'm\", 'everi', 'talk', 'annoy', '#downtonseasonthre', 'today', 'seem', 'shit', 'like', '#ilovemaggiesmith', 'baromet', 'appropri', 'nah', 'sneak', \"who'\", 'someth', 'F', 'activ', 'ugli', 'anyth', '10/22', '0.00', 'r', 'rain', 'run', 'gust', 'suppos', 'disser', 'mph', 'idk', '3.8', 'yu', '46.6'}\n"
     ]
    }
   ],
   "source": [
    "# query analysis collection\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "tweets_analysis = db[collection].find()\n",
    "for tweet in tweets_analysis:\n",
    "    print(tweet['full_text'])\n",
    "    print(tweet['tokens'])\n",
    "    print(tweet['timestamp'])\n",
    "    print('')\n",
    "    \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dump\n",
    "dump(['123', '456'], func_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tweet keyword filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-01 09:06:44 : Started query.\n\n2018-06-01 09:06:44 : Finished query. Returned 2 tweets.\n\n260244089985957888\n@augustushazel idk I'm just ugly or annoying or something\n{'coordinates': [-80.08961896, 42.09464892], 'type': 'Point'}\ndict_keys(['_id', 'coordinates', 'created_at', 'entities', 'full_text', 'id_str', 'place', 'location_id', 'borough', 'tokens'])\n\n260244093119102977\n10/22 @ 01:00 - Temperature 46.6F. Wind 1.5mph SW. Gust 3.8mph. Barometer 30.06in, Steady. Rain today 0.00in. Humidity 92%.\n{'coordinates': [-75.68444444, 39.695], 'type': 'Point'}\ndict_keys(['_id', 'coordinates', 'created_at', 'entities', 'full_text', 'id_str', 'place', 'location_id', 'borough', 'tokens'])\n\n"
     ]
    }
   ],
   "source": [
    "# query tweets containing specified tokens\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "db_name = 'sandy_test'\n",
    "collection = 'tweets_analysis_test'\n",
    "\n",
    "tokens = ['gust', 'humid', 'idk']\n",
    "tweets = ts.query_keyword(tokens=tokens, \n",
    "                          collection=collection, db_name=db_name, \n",
    "                          db_instance=db_instance, verbose=1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet['id_str'])\n",
    "    print(tweet['full_text'])\n",
    "    print(tweet['coordinates'])\n",
    "    print(tweet.keys())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-30 16:27:12 : Started query.\n",
      "\n",
      "2018-05-30 16:27:12 : Finished query. Returned 1 tweets.\n",
      "\n",
      "\"I suppose she has an appropriate costume for every activity...\" #ilovemaggiesmith #downtonseasonthree\n",
      "[{'indices': [65, 82], 'text': 'ilovemaggiesmith'}, {'indices': [83, 102], 'text': 'downtonseasonthree'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query tweets containing specified hashtags\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "db_name = 'sandy_test'\n",
    "collection = 'tweets_analysis_test'\n",
    "\n",
    "hashtags = ['downtonseasonthree', 'sandy']\n",
    "tweets = ts.query_keyword(hashtags=hashtags, \n",
    "                          collection=collection, db_name=db_name, \n",
    "                          db_instance=db_instance, verbose=1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet['full_text'])\n",
    "    print(tweet['entities']['hashtags'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-30 16:27:33 : Started query.\n",
      "\n",
      "2018-05-30 16:27:33 : Finished query. Returned 3 tweets.\n",
      "\n",
      "@augustushazel idk I'm just ugly or annoying or something\n",
      "\n",
      "\"I suppose she has an appropriate costume for every activity...\" #ilovemaggiesmith #downtonseasonthree\n",
      "\n",
      "10/22 @ 01:00 - Temperature 46.6F. Wind 1.5mph SW. Gust 3.8mph. Barometer 30.06in, Steady. Rain today 0.00in. Humidity 92%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query tweets containing specified tokens or hashtags\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "db_name = 'sandy_test'\n",
    "collection = 'tweets_analysis_test'\n",
    "\n",
    "tokens = ['gust', 'humid', 'idk']\n",
    "hashtags = ['test', 'downtonseasonthree']\n",
    "tweets = ts.query_keyword(tokens=tokens, hashtags=hashtags, \n",
    "                          collection=collection, db_name=db_name, \n",
    "                          db_instance=db_instance, verbose=1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet['full_text'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test group by summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 _id    borough  count  \\\n0  {'year': 2012, 'month': 10, 'day': 22, 'hour':...   Brooklyn      1   \n1  {'year': 2012, 'month': 10, 'day': 22, 'hour':...   Brooklyn      1   \n2  {'year': 2012, 'month': 10, 'day': 22, 'hour':...  Manhattan      1   \n3  {'year': 2012, 'month': 10, 'day': 22, 'hour':...  Manhattan      1   \n4  {'year': 2012, 'month': 10, 'day': 22, 'hour':...     Queens      1   \n\n                datetimeUTC  \n0 2012-10-22 13:00:00+00:00  \n1 2012-10-22 15:00:00+00:00  \n2 2012-10-22 15:00:00+00:00  \n3 2012-10-22 16:00:00+00:00  \n4 2012-10-22 23:00:00+00:00  \n"
     ]
    }
   ],
   "source": [
    "# test borough_day summary\n",
    "# can check datetime tz by finding first tweet in tweets_sandy\n",
    "title = 'sandy_test'\n",
    "analysis_collection = 'tweets_sandy'\n",
    "db_name = 'sandy'\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": {\n",
    "                \"year\": {\"$year\": \"$datetimeUTC\"},\n",
    "                \"month\": {\"$month\": \"$datetimeUTC\"},\n",
    "                \"day\": {\"$dayOfMonth\": \"$datetimeUTC\"},\n",
    "                \"hour\": {\"$hour\": \"$datetimeUTC\"},\n",
    "                \"borough\": \"$borough\"\n",
    "            },\n",
    "            \"datetimeUTC\": {\"$min\": {\"$dateFromParts\": {\n",
    "                \"year\": {\"$year\": \"$datetimeUTC\"},\n",
    "                \"month\": {\"$month\": \"$datetimeUTC\"},\n",
    "                \"day\": {\"$dayOfMonth\": \"$datetimeUTC\"},\n",
    "                \"hour\": {\"$hour\": \"$datetimeUTC\"}\n",
    "            }}},\n",
    "            \"borough\": {\"$first\": \"$borough\"},\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\"$sort\": {\"datetimeUTC\": 1, \"_id.borough\": 1}}\n",
    "]\n",
    "groups = list(db[analysis_collection].aggregate(pipeline))\n",
    "\n",
    "# convert to dataframe\n",
    "# groups = json.loads(df.to_json(orient='records', date_format='iso'))\n",
    "df = pd.DataFrame(groups)\n",
    "df['datetimeUTC'] = [datetime.tz_localize(tz='UTC') for datetime \n",
    "                     in df['datetimeUTC']]\n",
    "print(df.head())\n",
    "\n",
    "# groups = list(db[analysis_collection].aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test datetime group summaries with pandas and tzone conversion\n",
    "\n",
    "tweet_count_filter = 5  # by day\n",
    "# startdate = pd.Timestamp(2012, 10, 28, 0, 0, 0)  # inclusive\n",
    "# enddate = pd.Timestamp(2012, 11, 3, 0, 0, 0)  # exclusive\n",
    "startdate = pd.Timestamp('2012-10-28 00:00:00', \n",
    "                         tz='America/New_York')  # inclusive\n",
    "enddate = pd.Timestamp('2012-11-03 00:00:00', \n",
    "                       tz='America/New_York')  # exclusive\n",
    "\n",
    "# load sandy-related tweets filtered by dates\n",
    "df_sandy = ts.mongod_to_df({}, collection='tweets_sandy')\n",
    "df_sandy['datetimeUTC'] = [datetime.tz_localize(tz='UTC') for datetime\n",
    "                           in df_sandy['datetimeUTC']]\n",
    "df_sandy['datetime'] = [datetime.tz_convert('America/New_York') for datetime\n",
    "                        in df_sandy['datetimeUTC']]\n",
    "df_sandy = df_sandy.rename(columns={'location_id': 'zone'})\n",
    "df_sandy = df_sandy[['datetime', 'zone', 'tokens']]\n",
    "df_sandy = df_sandy.set_index('datetime')\n",
    "df_sandy = df_sandy.sort_index()\n",
    "df_sandy = df_sandy.loc[startdate:enddate]\n",
    "print('[min, max] sandy tweets datetime: [' + \n",
    "      str(min(df_sandy.index.get_level_values('datetime'))) + ', ' + \n",
    "      str(max(df_sandy.index.get_level_values('datetime'))) + '].')\n",
    "# group by datetime and zone\n",
    "df_sandy = df_sandy.reset_index()\n",
    "df_sandy['datetime'] = df_sandy['datetime'].dt.date\n",
    "df_sandy_group = df_sandy.groupby(['zone', 'datetime']).count()\n",
    "df_sandy_group = df_sandy_group.rename(columns={'tokens': 'sandy-tweets'})\n",
    "df_sandy_group = df_sandy_group[df_sandy_group['sandy-tweets'] >= tweet_count_filter] \n",
    "print('[min, max] sandy tweets: [' + \n",
    "      str(np.nanmin(df_sandy_group['sandy-tweets'])) + ', ' +\n",
    "      str(np.nanmax(df_sandy_group['sandy-tweets'])) + '].')\n",
    "print('')\n",
    "df_sandy_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': {'$gte': 5}, 'datetimeUTC': {'$lte': '2012-11-04T00:00:00', '$gte': '2012-10-28T00:00:00'}}\n         date    borough  count\n0  2012-10-28      Bronx    146\n1  2012-10-28   Brooklyn    656\n2  2012-10-28        EWR     28\n3  2012-10-28  Manhattan   1436\n4  2012-10-28     Queens    498\n    date    borough   count\n0  10-28      Bronx   146.0\n1  10-28   Brooklyn   656.0\n2  10-28        EWR    28.0\n3  10-28  Manhattan  1436.0\n4  10-28     Queens   498.0\n\n          date        borough  count\n25  2012-11-01       Brooklyn    226\n26  2012-11-01            EWR      6\n27  2012-11-01      Manhattan    438\n28  2012-11-01         Queens    136\n29  2012-11-01  Staten Island     52\n30  2012-11-02          Bronx     13\n31  2012-11-02       Brooklyn     53\n33  2012-11-02      Manhattan    105\n34  2012-11-02         Queens     30\n35  2012-11-02  Staten Island     41\n     date        borough  count\n32  11-02            EWR    NaN\n33  11-02      Manhattan  105.0\n34  11-02         Queens   30.0\n35  11-02  Staten Island   41.0\n36  11-03          Bronx    NaN\n37  11-03       Brooklyn    NaN\n38  11-03            EWR    NaN\n39  11-03      Manhattan    NaN\n40  11-03         Queens    NaN\n41  11-03  Staten Island    NaN\n"
     ]
    }
   ],
   "source": [
    "tweet_count_filter = 5\n",
    "#startdate = dt.datetime(2012, 10, 21, 0, 0, 0).isoformat()\n",
    "#enddate = dt.datetime(2012, 10, 28, 0, 0, 0).isoformat()\n",
    "startdate = dt.datetime(2012, 10, 28, 0, 0, 0).isoformat()\n",
    "enddate = dt.datetime(2012, 11, 4, 0, 0, 0).isoformat()\n",
    "query = {\n",
    "    \"count\": {\"$gte\": tweet_count_filter},\n",
    "    \"datetimeUTC\": {\"$lte\": enddate, \"$gte\": startdate}\n",
    "}\n",
    "print(query)\n",
    "df = ts.mongod_to_df(query, collection='borough_day_sandy')\n",
    "\n",
    "#daterange = ['10/22/2012', '10/27/2012']\n",
    "daterange = ['10/28/2012', '11/03/2012']\n",
    "#daterange = None\n",
    "\n",
    "# update dtypes and columns \n",
    "df['date'] = pd.to_datetime(df['datetimeUTC']).dt.date\n",
    "df = df[['date', 'borough', 'count']]\n",
    "\n",
    "# build full dataframe (all dates and all boroughs initialized with nans)\n",
    "if daterange:\n",
    "    dates = pd.date_range(start=daterange[0], end=daterange[1]).tolist()\n",
    "    dates = [pd.Timestamp.to_pydatetime(date).date() for date in dates]\n",
    "else:\n",
    "    dates = df['date'].unique()\n",
    "#print(dates)\n",
    "boroughs = sorted(df['borough'].unique())\n",
    "df_proc = pd.DataFrame({'date': [], 'borough': [], 'count': []})\n",
    "for date in dates:\n",
    "    for borough in boroughs:\n",
    "        df_temp = pd.DataFrame({'date': date,\n",
    "                                'borough': borough,\n",
    "                                'count': [np.nan]})\n",
    "        df_proc = df_proc.append(df_temp, ignore_index=True)\n",
    "\n",
    "# get matching indexes in df_proc of available data in df\n",
    "proc_indexes = [df_proc.index[(df_proc['date'] == date) & (\n",
    "                df_proc['borough'] == borough)].tolist()[0]\n",
    "                for date, borough in zip(list(df['date']),\n",
    "                                         list(df['borough']))]\n",
    "\n",
    "# update df_proc with available data in df\n",
    "df.index = proc_indexes\n",
    "df_proc.loc[proc_indexes, ['count']] = df['count']\n",
    "\n",
    "# reformat and rename columns\n",
    "df_proc['date'] = df_proc['date'].apply(lambda x: x.strftime('%m-%d'))\n",
    "\n",
    "# pivot dataframe for heat map visualization\n",
    "df_pivot = df_proc.pivot('borough', 'date', 'count')\n",
    "\n",
    "print(df[0:5])\n",
    "print(df_proc[0:5])\n",
    "print('')\n",
    "print(df[-10::])\n",
    "print(df_proc[-10::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for test scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260244087901413376, 260244088203403264, 260244088161439744, 260244088819945472, 260244089080004609, 260244089985957888, 260244092527706112, 260244093119102977, 260244093257515008, 260244094939439105]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_tweet_analysis\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "import twitterinfrastructure.twitter_sandy as ts\n",
    "import importlib\n",
    "importlib.reload(ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
