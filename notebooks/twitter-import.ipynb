{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for importing twitter data for hurricane sandy.\n",
    "\n",
    "The twitter dataset is from mdredze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T18:30:51.927849Z",
     "start_time": "2019-05-02T18:30:51.818074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\dev\\research\\socialsensing\\notebooks\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import twitterinfrastructure.twitter_sandy as ts\n",
    "import importlib\n",
    "importlib.reload(ts)\n",
    "\n",
    "#os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrate tweet IDs into tweets using Hydrator.\n",
    "1. Run the following cell to convert the raw mdredze sandy tweet ids file into an interim file of tweet ids in the format necessary to hydrate using Hydrator.\n",
    "1. Use [Hydrator](https://github.com/DocNow/hydrator) to hydrate the \"data/interim/sandy-tweetids.txt\" file. Hydrating on 03-14-2018 created a 13.3 GB json file with ??? tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T18:30:54.346921Z",
     "start_time": "2019-05-02T18:30:54.322985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-02 13:30:54 : Started converting tweet ids from data/raw/release-mdredze.txt to Hydrator format.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/interim/sandy-tweetids.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7d1e16b3e1d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwrite_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/interim/sandy-tweetids.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m num_tweets = ts.create_hydrator_tweetids(path=path, write_path=write_path, \n\u001b[1;32m----> 7\u001b[1;33m                                          filter_sandy=False, progressbar=False, verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\research\\socialsensing\\twitterinfrastructure\\twitter_sandy.py\u001b[0m in \u001b[0;36mcreate_hydrator_tweetids\u001b[1;34m(path, write_path, filter_sandy, progressbar, verbose)\u001b[0m\n\u001b[0;32m    392\u001b[0m                'format.'.format(path=path))\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m     \u001b[0mwrite_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;31m# loads and writes tweets line by line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/interim/sandy-tweetids.txt'"
     ]
    }
   ],
   "source": [
    "# create interim file with only tweet ids for hydration using Hydrator \n",
    "# (6,554,744 tweet ids, 124.5 MB)\n",
    "# takes ~1 min (3.1 GHz Intel Core i7, 16 GB 1867 MHz DDR3)\n",
    "path = \"data/raw/release-mdredze.txt\"\n",
    "write_path = \"data/interim/sandy-tweetids.txt\"\n",
    "num_tweets = ts.create_hydrator_tweetids(path=path, write_path=write_path, \n",
    "                                         filter_sandy=False, progressbar=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import hydrated tweets into mongodb database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T18:18:14.575310Z",
     "start_time": "2019-05-02T18:18:14.373886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-02 13:18:14 : Started inserting tweets from \"E:/Work/projects/twitterinfrastructure/data/processed/sandy-tweets-20180314.json\" to tweets collection in sandy database.\n",
      "\n",
      "2019-05-02 13:18:14 : Dropped tweets collection (if exists).\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811c6fa5e06b4bd1b3277bfff86d12e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 7105: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-9e6f35814e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m insert_num = ts.insert_tweets(path, collection=collection, db_name=db_name, \n\u001b[0;32m     10\u001b[0m                               \u001b[0mdb_instance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdb_instance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                               overwrite=True, verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\research\\socialsensing\\twitterinfrastructure\\twitter_sandy.py\u001b[0m in \u001b[0;36minsert_tweets\u001b[1;34m(path, collection, db_name, db_instance, progressbar, overwrite, verbose)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mfile_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m             \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\research\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\research\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1003\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\research\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8f in position 7105: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# import tweets (4799665 tweets out of 4799665 lines, 12.2 GB total doc size)\n",
    "# takes ~ 40 mins (3.1 GHz Intel Core i7, 16 GB 1867 MHz DDR3)\n",
    "# path = 'data/processed/sandy-tweets-20180314.json'\n",
    "path = 'E:/Work/projects/twitterinfrastructure/data/processed/sandy-tweets-20180314.json'\n",
    "collection = 'tweets'\n",
    "db_name = 'sandy'\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "\n",
    "insert_num = ts.insert_tweets(path, collection=collection, db_name=db_name, \n",
    "                              db_instance=db_instance, progressbar=True,\n",
    "                              overwrite=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import taxi_zones GeoJSON into mongodb database.\n",
    "\n",
    "1. Open terminal.\n",
    "1. Change to the twitterinfrastructure project home directory. For example, run the following (based on my directory structure):\n",
    "\n",
    "\t$ cd Documents/projects/twitterinfrastructure\n",
    "\n",
    "1. Use mongoimport to import the taxi_zones_crs4326_mod.geojson into the database by running the following in terminal (not mongodb shell). Be aware of double dash lines in front of db, collection, file, and jsonArray arguments).\n",
    "\n",
    "\t$ mongoimport --db sandy --collection taxi_zones --file \"data/processed/taxi_zones_crs4326_mod.geojson\" --jsonArray\n",
    "\n",
    "1. Run the following cell to create a geosphere index in the taxi_zones collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263 taxi zones found in imported taxi_zones GeoJSON file.\n"
     ]
    }
   ],
   "source": [
    "# create geosphere index in taxi_zones collection\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "db_name = 'sandy'\n",
    "zones_collection = 'taxi_zones'\n",
    "#db_name = 'sandy_test'\n",
    "#zones_collection = 'taxi_zones_test'\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "db[zones_collection].create_index([(\"geometry\", pymongo.GEOSPHERE)])\n",
    "\n",
    "zones = db[zones_collection].find()\n",
    "print('{count} taxi zones found in imported taxi_zones GeoJSON file.'.format(\n",
    "    count=zones.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import nyiso_zones GeoJSON into mongodb database.\n",
    "\n",
    "1. Open terminal.\n",
    "1. Change to the twitterinfrastructure project home directory. For example, run the following (based on my directory structure):\n",
    "\n",
    "\t$ cd Documents/projects/twitterinfrastructure\n",
    "\n",
    "1. Use mongoimport to import the 'nyiso-zones-crs4326-mod.geojson' file into the database by running the following in terminal (not mongodb shell). Be aware of double dash lines in front of db, collection, file, and jsonArray arguments. Make sure you delete any existing nyiso_zones collection in the database (the command will append, not overwrite).\n",
    "\n",
    "\tThis geojson was created by manually querying and copying nyiso zone geojsons from [here](https://services1.arcgis.com/Lsfphzk53dXVltQC/arcgis/rest/services/NYISO_Zones/FeatureServer/0/query?outFields=*&where=1%3D1) (linked from [here](https://hub.arcgis.com/items/3a510da542c74537b268657f63dc2ce4)) to the 'data/raw/nyiso/' directory. Those individual zone geojsons were then combined into the 'nyiso.geojson' file and loaded into qgis3 (version 3.2, using the 'Add Vector Layer' option, individual zones were visualized by adjusting symbology of the layer properties to be categorized). The layer was then exported to a geojson file using qgis3 (with the EPSG:4326 crs).\n",
    "\n",
    "\t$ mongoimport --db sandy --collection nyiso_zones --file \"data/processed/nyiso-zones-crs4326-mod.geojson\" --jsonArray\n",
    "\n",
    "1. Run the following cell to create a geosphere index in the nyiso_zones collection and add the properties.zone_id field to each zone in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 nyiso zones found in imported nyiso_zones GeoJSON file.\n"
     ]
    }
   ],
   "source": [
    "# create geosphere index in nyiso_zones collection\n",
    "db_instance = 'mongodb://localhost:27017/'\n",
    "db_name = 'sandy'\n",
    "zones_collection = 'nyiso_zones'\n",
    "client = pymongo.MongoClient(db_instance)\n",
    "db = client[db_name]\n",
    "db[zones_collection].create_index([(\"geometry\", pymongo.GEOSPHERE)])\n",
    "zones = db[zones_collection].find()\n",
    "print('{count} nyiso zones found in imported nyiso_zones GeoJSON file.'.format(\n",
    "    count=zones.count()))\n",
    "\n",
    "# add zone_id to nyiso_zones collection\n",
    "zones_path = 'data/raw/nyiso/nyiso-zones.csv'\n",
    "df = pd.read_csv(zones_path)\n",
    "for abbrev, zone_id in zip(df['abbrev'], df['zone_id']):\n",
    "    db[zones_collection].update_one(\n",
    "        {\"properties.Zone\": abbrev},\n",
    "        {\"$set\": {\"properties.zone_id\": zone_id}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
